ставим requirements.txt через virtualenv

Kопируем шаблон inventory/sample as inventory/mycluster

Генерируем inventory
declare -a IPS=(172.16.2.2 172.16.2.3 172.16.2.4 172.16.2.5 172.16.2.6)
~]# CONFIG_FILE=inventory/mycluster/hosts.ini python36 contrib/inventory_builder/inventory.py ${IPS[@]}


Возможно придется добавить в host.ini

ansible_connection=ssh
ansible_become=true
ansible_become_user=root
ansible_user= vagrant
ansible_ssh_pass=vagrant
ansible_python_interpreter=/usr/bin/python3

Cтавим
ansible-playbook -i inventory/mycluster/hosts.ini cluster.yml

Расширяем после изменения инвентори
ansible-playbook -i inventory/mycluster/hosts.ini scale.yml

Удаляем ноду
ansible-playbook -i inventory/mycluster/hosts.ini remove-node.yml

Удаляем установку
ansible-playbook -i inventory/mycluster/hosts.ini reset.yml


__________________

Настройка kubectl
копируем с мастера /etc/kubernetes/admin.conf ~/.kube/config

Чек на память:
kubespray/roles/kubernetes/preinstall/tasks/0020-verify-settings.yml

Поставить helm:
inventory/sample/group_vars/k8s-cluster/addons.yml
helm_enabled: true # устанавливаем helm на ноды
local_volume_provisioner_enabled: true # активируем local volume provisioner
ingress_nginx_enabled: true # активируем ingress controller
